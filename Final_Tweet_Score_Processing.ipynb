{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Daniel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Daniel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import py_stringmatching.tokenizer as tk\n",
    "import string\n",
    "import nltk\n",
    "import pickle\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import sys\n",
    "\n",
    "#-----------------------------------------------------------------------------#\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#-----------------------------------------------------------------------------#\n",
    "\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "import pandas.io.sql as psql\n",
    "import psycopg2 as pg\n",
    "from datetime import datetime\n",
    "\n",
    "# First time do: pip install pymongo\n",
    "from pymongo import MongoClient\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def start_up():\n",
    "    # Connect to database\n",
    "    conn = pg.connect(\n",
    "        database=\"dse203\", \n",
    "        user=\"postgres\", \n",
    "        password=\"newPass\"\n",
    "    )\n",
    "    \n",
    "    return conn\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing tweet text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stemmingLambda(ws_tok,x):\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    retVal = \"\"\n",
    "    x = ws_tok.tokenize(x)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    for w in x:\n",
    "        if w not in stop_words:\n",
    "            retVal += porter_stemmer.stem(w)\n",
    "            retVal += \" \"\n",
    "    return retVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_tweets(orig_tweet_texts):\n",
    "    table = str.maketrans({key: None for key in string.punctuation})\n",
    "\n",
    "    ws_tok = tk.whitespace_tokenizer.WhitespaceTokenizer()\n",
    "    processed_tweet_texts = list(map(lambda x: stemmingLambda(ws_tok,x.translate(table).lower()), orig_tweet_texts)) \n",
    "    processed_tweet_texts = list(map(lambda x: set(x.strip().split(' ')),processed_tweet_texts))\n",
    "    return processed_tweet_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating set intersection for reuters lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom nltk.corpus import reuters\\nreuters_lexicon_set = set()\\nfor file in reuters.fileids():\\n    curr_doc = reuters.raw(file)\\n    curr_doc = curr_doc.translate(table).lower()\\n    curr_doc = stemmingLambda(curr_doc)\\n    tokens = curr_doc.strip().split(\\' \\')\\n    for token in tokens:\\n        reuters_lexicon_set.add(token)\\n        \\n#Pickling processed lexicon\\nwith open(\"reuters_processed_lexicon.pickle\", \"wb\") as file:   \\n    pickle.dump(reuters_lexicon_set, file)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Used first time through\n",
    "\"\"\"\n",
    "from nltk.corpus import reuters\n",
    "reuters_lexicon_set = set()\n",
    "for file in reuters.fileids():\n",
    "    curr_doc = reuters.raw(file)\n",
    "    curr_doc = curr_doc.translate(table).lower()\n",
    "    curr_doc = stemmingLambda(curr_doc)\n",
    "    tokens = curr_doc.strip().split(' ')\n",
    "    for token in tokens:\n",
    "        reuters_lexicon_set.add(token)\n",
    "        \n",
    "#Pickling processed lexicon\n",
    "with open(\"reuters_processed_lexicon.pickle\", \"wb\") as file:   \n",
    "    pickle.dump(reuters_lexicon_set, file)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_newsworthiness(processed_tweet_texts):\n",
    "    # Unpickling\n",
    "    with open(\"reuters_processed_lexicon.pickle\", \"rb\") as fp:   \n",
    "        reuters_lexicon_set = pickle.load(fp)\n",
    "\n",
    "    reuters_intersections = []\n",
    "\n",
    "    for tweet_text in processed_tweet_texts:\n",
    "        matches = len(tweet_text.intersection(reuters_lexicon_set))/len(tweet_text)\n",
    "        reuters_intersections.append(matches)\n",
    "        \n",
    "    return reuters_intersections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate set intersection for article lexicion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_uniqueness(processed_tweet_texts,topic_num,time_partition,conn):\n",
    "    # Obtain article group lexicon\n",
    "    corpus_query = \"\"\"\n",
    "    SELECT corpus FROM topic_corpus WHERE topic=\"\"\" + topic_num + \"\"\" AND date='\"\"\" + time_partition + \"\"\"'\"\"\"\n",
    "    \n",
    "    topic_corpus = pd.read_sql(corpus_query, conn)\n",
    "    topic_corpus = topic_corpus['corpus'].values[0]\n",
    "    topic_corpus_set = set(topic_corpus.split(' '))\n",
    "    \n",
    "    article_topic_intersections = []\n",
    "    for tweet_text in processed_tweet_texts:\n",
    "        matches = 1 - ((len(tweet_text.intersection(topic_corpus_set))/len(tweet_text)))\n",
    "        article_topic_intersections.append(matches)\n",
    "        \n",
    "    return article_topic_intersections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading scores to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def upload_scores(orig_tweet_texts,topic_num,time_partition,newsworthiness,uniqueness):\n",
    "    newsworthiness_dataframe = pd.DataFrame({'text':orig_tweet_texts})\n",
    "    newsworthiness_dataframe['topic'] = topic_num\n",
    "    newsworthiness_dataframe['partition'] = time_partition\n",
    "    newsworthiness_dataframe['newsworthiness'] = newsworthiness\n",
    "    newsworthiness_dataframe['uniqueness'] = uniqueness\n",
    "        \n",
    "    engine = create_engine('postgresql://postgres:newPass@localhost:5432/dse203', echo=False)\n",
    "\n",
    "    newsworthiness_dataframe.to_sql('tweet_text_scores', con=engine, if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run(tweet_list,topic,partition):\n",
    "    if len(tweet_list) == 0:\n",
    "        return\n",
    "    orig_tweet_texts = tweet_list\n",
    "    topic_num = topic\n",
    "    time_partition = partition\n",
    "    conn = start_up()\n",
    "    processed_tweets = process_tweets(orig_tweet_texts)\n",
    "    newsworthiness = get_newsworthiness(processed_tweets)\n",
    "    uniqueness = get_uniqueness(processed_tweets,topic_num,time_partition,conn)\n",
    "    upload_scores(orig_tweet_texts,topic_num,time_partition,newsworthiness,uniqueness)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
